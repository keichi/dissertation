\chapter*{Summary}

% 背景
Recent breakthrough in science has significantly benefited from High
Performance Computing~(HPC). The majority of HPC systems today adopt cluster
architecture to achieve their massive computing performance. A cluster is a
system of computers connected through a high-performance network refered to as
an interconnect. Computers composing a cluster work collectively by
communicating with each other over the interconnect.

% 問題
The inter-process communication of MPI applications shows a distinctive
pattern. This pattern varies depending on the application and originates from
the mathematical model, discretization method, and parallelization strategy.
The design of an interconnect could be highly optimized for an
application by taking the communication pattern of applications into account.
However, this approach is infeasible when designing a
real-world cluster. This is because HPC systems are shared by many users
where each user runs various applications on the cluster. Therefore, in
contrast to the application-dependent communication pattern, the interconnect
is inherently designed in an application-agnostic manner.
As a result, an imbalance of the packet flow in the interconnect can occur
under a certain combination of communication pattern and interconnect.
The imbalance can lead to the concentration of traffic on a link in the
interconnect and slow down of MPI communication that uses the link. The
degraded MPI communication can ultimately lead to a serious degradation of
total application performance.

% 目的
To this date, however, there has been little studies on adapting the
interconnect to the communication pattern of applications. This is mostly
because it has been assumed that flexibly and dynamically reconfiguring the
interconnect at runtime is infeasible. However, this assumption might not hold
anymore with the recent emergence of programmable networking architectures
such as Software-Defined Networking~(SDN) that
allows reconfiguring the interconnect on-the-fly.
This dissertation aims to overcome this shortcoming of
conventional application-agnostic interconnects by establishing a programmable
interconnect control that dynamically controls the packet flow in the
interconnect based on the communication pattern of applications.

% 課題
Following three challenges must be tackled to materialize this concept: (1)
analyzing the packet flow in the interconnect, (2) accelerating MPI
communication by dynamically controlling the packet flow in the interconnect,
and (3) coordinating the execution of application and interconnect control.

% 提案1
To address the first challenge, we propose a toolset for analyzing the packet
flow in the interconnect. The packet flow generated in the interconnect highly
depends on parameters such as the communication pattern of application,
interconnect design and cluster configuration. When designing and implementing
an efficient programmable interconnect control, researchers need to conduct a
systematic analysis over many combinations of these parameters. Since
performing such analysis on a physical cluster is time consuming, we utilize
simulation to facilitate the analysis. PFAnalyzer is a pair of two tools:
PFSim, an interconnect simulator specialized for programmable interconnects,
and PFProf, a profiler to collect communication pattern from MPI applications.
PFSim allows interconnect researchers and designers to investigate congestion
in the interconnect for an arbitrary cluster configuration and a set of
communication patterns collected by PFProf. We evaluate the accuracy of the
simulation results obtained from PFSim and demonstrate how PFAnalyzer can be
used to analyze the effect of programmable interconnect control.

% 提案2
To address the second challenge, we propose a framework to accelerate MPI
collectives by dynamically controlling the packet flow in the interconnect.
Out of the communication primitives provided by MPI, we focus on accelerating
collective communication because it consumes a significant fraction of the
execution time of applications. We integrate the network programmability
provided by Software-Defined Networking into MPI collectives so that
collectives are able to effectively utilize the bandwidth of the interconnect.
In particular, we aim to reduce the execution time of MPI\_Allreduce, which is
a frequently used MPI collective communication in many simulation codes. We
evaluate the speedup of MPI\_Allreduce when using our collective acceleration
framework.

% 提案3
To address the third challenge, we propose a software-defined coordination
mechanism that performs interconnect control in synchronization with the
execution of applications. In a real-world application, the communication
pattern changes with the execution of application. Therefore, a mechanism to
coordinate packet flow control and execution of application is essential.
UnisonFlow is a kernel-assisted mechanism that realizes such coordination in a
per-packet basis while maintaining significantly low overhead. We verify that
the interconnect control can be successfully performed in synchronization with
the execution of the application and evaluate the overhead imposed by the
coordination mechanism.

Finally, we conclude this dissertation and discuss future work.
