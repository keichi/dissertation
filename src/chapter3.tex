\chapter{Accelerated MPI Collective Using Software-Defined Networking}\label{sec:iii}

\section{Introduction}\label{sec:iii-introduction}

As described in Section~\ref{sec:i-mpi}, communication primitives defined in
MPI can be roughly categorized into point-to-point communication, collective
communication and one-sided communication. Out of these three categories, we
particularly focus on accelerating collective communication because of its
significant impact to the application performance. In fact, a recent analysis
of MPI usage on a production HPC system revealed that approximately 34\% of
the total core-hours of the system are expended in MPI communication and 66\%
of the total MPI communication time is spent in collective
communication~\autocite{Chunduri2018}. This clearly indicates that reducing
the time of collective communication is of great importance.

MPI collectives require intensive communication among multiple compute node
pairs. However, while some compute node pairs communicate less, some pairs
have to communicate much more than other pairs. Because of this imbalance,
even in the case where the interconnect of a cluster system has multiple
redundant routes between compute nodes, packet flow generated from MPI
collectives could collide on a single link of the interconnect without any
control of packet flow.

In this chapter, we aim at accelerating MPI communication by dynamically
controlling the packet flow in the interconnect. In particular, we target a
cluster system with interconnect that contains multiple redundant routes;
specifically, we focus on fat-tree interconnects. We design and implement a
framework that effectively makes use of redundant routes by dynamically
controlling the packet flow in the interconnect using SDN described in
Section~\ref{sec:i-sdn}.

The rest of this chapter is organized as follows:
Section~\ref{sec:iii-background} reviews conventional traffic balancing
methods and analyzes their problems. Subsequently, the goal of this chapter is
clarified. In Section~\ref{sec:iii-proposal}, the design and implementation of
the proposed architecture of SDN-enhanced MPI\_Allreduce is presented. In
Section~\ref{sec:iii-evaluation}, an evaluation on a cluster system is
conducted to verify the feasibility of the proposed framework.
Section~\ref{sec:iii-related-work} discusses related works.
Section~\ref{sec:iii-conclusion} concludes this chapter and discusses
challenges for further improving the practicality of our proposed framework.

\section{Research Objective}\label{sec:iii-background}

MPI collective operations require much simultaneous communication among
multiple process pairs. However, while some compute node pairs
communicate less, some pairs have to communicate much more than other
pairs. When the underlying network of the cluster system that
interconnects with the compute nodes has a full-bisection bandwidth,
the bias of source and destination in communication should not be a
problem although structuring this type of a network is hard to scale out
due to economic and physical restrictions~\autocite{Al-Fares2008}. Under the
assumption that the network is oversubscribed, the shortage of available
bandwidth could give rise to a serious problem.

Suppose a cluster system of four compute nodes interconnected with a
two-tier fat-tree topology as illustrated in Fig.~\ref{fig:problem-routing1}.
Fat-tree is a network topology that has multiple redundant routes between the
upper layer switches and lower layer switches. By distributing traffic among
these redundant routes, it is possible to gain higher bisection bandwidth
compared to a simple tree topology.

The traffic load balancing algorithm becomes important on such interconnects.
For example, suppose
compute node 1 is sending data to node 3, and node 2 is sending
different data to node 4 at the same moment. If those two communications
are routed within the exact same route, link contention could happen, as
depicted in Fig.~\ref{fig:problem-routing1}. When these two
communications are routed so that they make use of the redundant routes
as depicted in Fig.~\ref{fig:problem-routing2}, link contention can be
avoided.

\begin{figure}
    \centering
    \includegraphics[width=.6\linewidth]{problem-routing1.png}
    \caption{Link Contention on Fat-tree}%
    \label{fig:problem-routing1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.6\linewidth]{problem-routing2.png}
    \caption{Load Balancing Routes on Fat-tree}%
    \label{fig:problem-routing2}
\end{figure}

Various algorithms for balancing traffic in a network with redundant
routes have been proposed. Equal-Cost Multi-Path routing (ECMP)~\autocite{ecmp} is
a standardized load balancing strategy mainly used in L3 switches. For each
communication between two hosts, if multiple equal cost routes are available,
ECMP selects one route from among them. The decision of which route to use is
based on the header fields (\emph{e.g.} source and destination addresses) of
each packet. A hash function is applied to the header fields to generate the
corresponding hash value for the header fields, where every value in the hash
value space are evenly assigned to one of the equal cost routes.

InfiniBand~\autocite{Buyya2009} is a computer network communication link
commonly used in the area of HPC and a data center. InfiniBand supports
multiple routing methods. One of those methods is a min-hop routing
algorithm, which calculates the minimum hop route between every
compute node pair. If multiple minimum hop routes for a single
compute node pair are available, the algorithm assigns a route so that
usage among links is equalized.

The problem of these existing conventional load balancing mechanisms is
that they do not consider the communication pattern of MPI
applications. In general, MPI applications cannot retrieve usage
information of the underlying network or control it. MPI communication
does not occur evenly among compute nodes, and its sources and
destinations are biased. This may cause inequality of link usage that
decreases available bandwidth between compute nodes. Ultimately, this
decrease of available bandwidth can lead to performance degradation of MPI
applications.

From our observations above, we believe that an application-aware network
control mechanism which recognizes the communication pattern needed by MPI
collectives and effectively uses the bandwidth of each link by distributing
the traffic among redundant routes is essential. Therefore, in this chapter,
we leverage Software-Defined Networking (SDN) to enable such dynamic control
of packet control depending on the communication pattern of MPI applications.

In particular, we focus on accelerating MPI\_Allreduce.
MPI\_Allreduce is one of the most frequently used and time-consuming
collective communication functions of MPI\@~\cite{Chunduri2018}.
This collective communication reduces values from all
processes with an operator and broadcasts the result to every process.
More specifically, suppose there are $n$ processes in a communicator and
each process with rank $i$ ($0 \leq i < n$) has $l$ values $x_i^0, x_i^1,
\dots, x_i^l$. After MPI\_Allreduce is completed, all processes have values
$y^0, y^1, \dots, y^l$ where $y^j = x_0^j \oplus x_1^j \oplus \dots \oplus
x_{n-1}^j$ and $\oplus$ is the operator used for reduction. The operator can
be any user-defined associative operator or one of the pre-defined operators
such as sum, product, or maximum. Figure~\ref{fig:mpi-allreduce} shows an
example where four processes each having five values call MPI\_Allreduce with
the sum operator.

\begin{figure}
    \centering
    \includegraphics{mpi-allreduce}
    \caption{Effect of MPI\_Allreduce}%
    \label{fig:mpi-allreduce}
\end{figure}

One of the use cases of MPI\_Allreduce is parallel Conjugate Gradient~(CG)
method. CG method is an iterative algorithm to solve a system of linear
equations $Ax = b$ whose coefficient matrix $A$ is positive-definite and
symmetric. In parallel CG method, a significant amount of time is spent in
MPI\_Allreduce to compute the inner product of
vectors~\autocite{Kandalla2012}. Another user case is parallel Stochastic
Gradient Descent~(SGD). SGD is a continuous optimization algorithm that
minimizes an objective function $f$ parameterized by $w$ for a given set of
input $S$. SGD incrementally updates $w$ in the following way: $w^{t+1}=w^t-
\eta \nabla f(w^t; z^t)$ where $w^t$ is the parameter for the $t$-th
iteration, $z^t$ is an input data randomly sampled from $S$, and $\eta$ is a
small constant. In parallel SGD, the gradient $\nabla f(w^t; z^t)$ is computed
in parallel by each process using different samples. Subsequently,
MPI\_Allreduce is used to compute the average of the individual gradients
computed by all processes.

We try to accelerate MPI\_Allreduce on a cluster system with a fat-tree
interconnect. This study integrates the dynamic network controllability of SDN
with MPI in order to avoid link contention and evaluates the performance of
MPI\_Allreduced based on our proposed framework.

\section{Proposal}\label{sec:iii-proposal}

In this section, we present our proposed framework for accelerating MPI
collectives. First, we outline the basic idea behind the proposed framework.
After that, the design and implementation of the framework is described in
detail.

\subsection{Basic Idea}

The basic idea behind the proposed framework is to improve the
inefficient communication in MPI in order to enhance the performance of
MPI applications. The fact that the MPI application cannot retrieve
usage information of the underlying network results in a potential
inefficiency in terms of communication. In this chapter, we focus on the
inequality of the link usage among the interconnect of a cluster system. Since
the bandwidth of links is limited, such inequality results in contention in
heavy traffic links.

In this chapter, we use redundant routes of the interconnect to ease the
inequality of link usage, based on an assumption that the cluster system has a
fat-tree interconnect. Through this traffic distribution, contention in the
links is expected to ease, which speeds up communication.

\subsection{Design of Collective Acceleration Framework Using SDN}

The proposed framework comprises of three modules. These three modules are:
SDN controller, LLDP daemon and SDN MPI library. They are deployed onto a
cluster system as illustrated in Fig.~\ref{fig:proposal-placement}. The SDN
controller is deployed onto a management node, which is a computer that is not
involved in the actual computation of the MPI application, but is used for
controlling the whole cluster system such as in deploying jobs to the system.
LLDP daemon runs in the background on all compute nodes. The SDN MPI library
is not a stand-alone application, but a static library that must be linked to
MPI applications at compile time. Each compute node can run one or more MPI
processes, but in this chapter, we assume each compute node runs only a single
MPI process.

\begin{figure}
    \centering
    \includegraphics{sdn-mpi-arch}
    \caption{Placement of the Modules Composing SDN-enhanced MPI\_Allreduce}%
    \label{fig:proposal-placement}
\end{figure}

The interaction among these modules is roughly divided into two phases: the
initialization phase at the MPI application startup and the main phase at each
MPI collective call. Figure~\ref{fig:proposal-sequence} is a UML sequence
diagram that illustrates how the modules cooperate with each other. MPI\_Init
is an MPI function that initializes the MPI execution environment, which must
be called on the application startup. After MPI\_Init finishes, all MPI
processes notify their own IP address and MPI \emph{rank} number to the SDN
controller. This information obtained from MPI processes are held by the
controller until the execution of the MPI application finishes.

\begin{figure}
    \centering
    \includegraphics{sdn-mpi-sequence}
    \caption{Sequence Diagram of the Proposed Framework}%
    \label{fig:proposal-sequence}
\end{figure}

When an MPI collective is called, the rank 0 process generates the
\emph{communication pattern} of the MPI collective. This communication pattern is
a set of sender process and receiver process pairs during the MPI collective
communication. After the communication pattern is generated, this set is sent
to the SDN controller by the rank 0 process. As soon as the SDN controller
receives the communication pattern, it generates a route for each
sender-receiver pair. Subsequently, the SDN controller programs each
SDN-enabled switch so that the MPI packets are routed along the pre-generated
routes. After the entire communication pattern is processed, the MPI
collective is called to start the actual data transfer and computation for the
collective operation.

\subsection{Implementation of Collective Acceleration Framework Using SDN}

To realize the proposed framework, we have developed three modules which
work as an integrated system: LLDP daemon, SDN controller and SDN MPI library.
This section explains the implementation of these three modules in detail.

\subsubsection{LLDP Daemon}

Each compute node runs an LLDP~(Link Layer Discovery
Protocol)~\autocite{lldp} daemon in the background. This daemon is designed to
emit LLDP packets containing hardware information periodically, which are
received by the SDN-enabled switches and used for topology discovery. Some
LLDP daemon implementations already exist. However we have developed a new,
minimal daemon to easily add and tweak features so that it can cooperate with
the other programs composing the whole system.

This daemon detects all available network interfaces installed on a
computer and queries its interface index, MAC address and IP address.
This information is packed into a single LLDP packet and sent out from
each network interface periodically. The interval is set to 1 second in
this prototypical implementation to speed up topology discovery.
However, it can be a longer period in practical systems so that its
topology does not change frequently.

As described above, the LLDP daemon emits a few hundred byte long
packets to the network every second. We consider this traffic is as
small enough so that it does not cause serious side effects on the
actual MPI process, for instance taking CPU time away from the
application or consuming too much bandwidth that could affect the MPI
communication. Generation of such LLDP packets is also not difficult
work for the computer, so we consider the impact to the application is
not severe.

\subsubsection{SDN Controller}

The SDN controller is developed on top  of Trema~\autocite{trema}, a framework
designed for easily developing OpenFlow controllers in the Ruby language, and
has four core functionalities:

\begin{itemize}
\item Generating routing for MPI collectives to mitigate link contention and
    installing generated routing to SDN-enabled switches
\item Detecting the topology and usage of the interconnect using LLDP
\item Responding to Address Resolution Protocol~(ARP) requests from compute
    nodes to avoid broadcast storms
\item Routing of non-MPI traffic such as ICMP and SSH
\end{itemize}

The first functionality is topology detection. How detection is
performed is shown in Fig.~\ref{fig:lldp}. The controller periodically
requests every switch to send out an LLDP packet from each of their physical
ports (step 1 in Fig.~\ref{fig:lldp}). This LLDP packet contains two kinds of
information: datapath ID (a number that uniquely distinguishes the switches)
and port number (port index where the packet is sent out). Moreover, all
compute nodes also emit LLDP packets from the LLDP daemon described above.
The controller is notified when a LLDP packet arrives at the controller. After
that, it parses the packet to obtain information on the packet's origin, to
examine whether the packet came from a compute node or an SDN-enabled
switch, and its MAC address/Datapath ID and Interface index/Port number (step
2 in Fig.~\ref{fig:lldp}). Using this information from its neighbors, an
adjacency list is generated (step 3 in Fig.~\ref{fig:lldp}). From this
adjacency list, a network topology graph is constructed, which is used in the
route generation and routing. If the packet is from a compute node, the
source MAC address and IP address are registered in a MAC/IP address
translation table used in the ARP responding functionality.

\begin{figure}
    \centering
    \includegraphics{lldp}
    \caption{Topology Detection using LLDP}%
    \label{fig:lldp}
\end{figure}

The second functionality is replying to ARP requests. An ARP request is
a L2 broadcast packet, so it causes a \emph{broadcast storm} problem
inside a network that contains a loop inside it. Since the target of
this chapter is a network that has redundant routes, it always has a loop
in it. Thus, the controller instructs the switches to reply to the
received ARP request, instead of the compute node that has the
corresponding IP address. The IP address that corresponds to the MAC
address is obtained from the MAC/IP address translation table described above.

The third functionality is route generation and installation for MPI
collective communication. The route generation algorithm is implemented as a
pluggable module so that different algorithms can be specifically tailored for
each MPI collective.

Since we focus on accelerating MPI\_Allreduce in this research, we designed
and implemented a route generation algorithm for MPI\_Allreduce. This
algorithm controls the packet flow of MPI\_Allreduce so that other MPI
communication is not affected. When an MPI application calls MPI\_Allreduce,
the rank 0 process computes the \emph{communication pattern} of
MPI\_Allreduce.

The route searching algorithm used here for its speed and simplicity is the
\emph{Dijkstra} algorithm. The Dijkstra algorithm solves the shortest route
problem for a graph with non-negative link weights. In this algorithm, link
weights are considered the number of total routes that go through that link.
Under this assumption, the shortest route means a route that shares links with
other routes. For each pair of sender processes and receiver processes, a
route is generated with the Dijkstra algorithm. After that, the weight of all
links contained in the generated route is incremented. This procedure is
repeated for all sender and receiver process pairs. This algorithm selects the
route that shares links with other routes from multiple possible routes, which
results in a distribution of routes.

Algorithm~\ref{lst:code-generate-route} is a pseudo-code for the algorithm.
First, an empty array \emph{routes} is initialized. After that, the
route search is performed for each sender-receiver pair. The resulting
route is added to \emph{routes} and the link weight (which is the number
of total routes that use that link) of each links is incremented. After
all routes have been generated, these routes are installed to the
SDN-enabled switches.

\begin{algorithm}
    \caption{Pseudocode of Route Generation.}%
    \label{lst:code-generate-route}
    \begin{algorithmic}
        \STATE routes = emptyArray();
        \STATE nodes = nodes in the topology graph;
        \STATE links = links in the topology graph;
        \FORALL{sender-receiver pair}
            \STATE route = dijkstra(nodes, links, sender, receiver);
            \STATE routes.push(route);
            \FORALL{link in route}
                \STATE link.weight += 1;
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

The fourth functionality is route generation and installation for
non-MPI traffic. For non-MPI traffic such as ICMP and SSH packets, the
SDN controller generates the minimum hop routes between compute nodes
and installs them on demand.

\subsubsection{SDN MPI Library}

An MPI application that wants to use the proposed framework must be linked
with the SDN MPI library. This library contains two classes of functions:
SDN\_MPI\_Init, which is the initialization function for the library, and
MPI collective functions prefixed with SDN\_MPI\_, which replace the
conventional MPI collectives with their SDN-enhanced versions.

The application is required to call SDN\_MPI\_Init when it launches. This
function opens a TCP connection with the SDN controller and notifies the IP
address and MPI rank number of the process that has called itself.

SDN\_MPI collectives are called by the application when it needs to perform
collective communication. Each SDN\_MPI collective generates the communication
pattern (set of sender process and receiver process pairs during the
collective communication) for the MPI application and sends the pattern to the
SDN controller.

Several algorithms to realize the Allreduce operation have been proposed.
Here, we focus on \emph{recursive doubling}~\autocite{Thakur2005}, since it
requires more inter-node communication compared with other algorithms, which
means more room for optimization in terms of communication.
Figure~\ref{fig:recursive-doubling} shows how the recursive doubling algorithm
works. In this context, the \emph{distance} between two MPI processes is
defined as the absolute value of the difference of rank numbers. In the first
step, processes that are 1 distance apart exchange their data and perform the
reduction operation between the data that the process has originally held and
with the just exchanged data. In the second step, processes that are 2
distance apart exchange their data, and in step \(p\), processes that are
\(2^{p - 1}\) distance apart exchange their data. The SDN MPI
library memorizes all process pairs that have to communicate and exchange data
by following each step of the recursive doubling algorithm. For each of those
pairs, the library notifies the SDN controller to prepare each route.

\begin{figure}
    \centering
    \includegraphics{recursive-doubling}
    \caption{Recursive Doubling Algorithm}%
    \label{fig:recursive-doubling}
\end{figure}

\section{Evaluation}\label{sec:iii-evaluation}

\subsection{Experimental Environment}

An experiment was conducted to compare the execution time of MPI\_Allreduce
accelerated with our proposed framework against conventional
MPI\_Allreduce. The experimental environment is illustrated in
Fig.~\ref{fig:experiment-environment}. This experiment was performed on
a real cluster system consisting of 28 compute nodes and 6 SDN-enabled
switches, which form a two-tier fat-tree topology. The compute nodes and
SDN-enabled switches were all connected through 1 Gigabit Ethernet links;
hence the interconnect was oversubscribed.

In addition to the network connecting the compute node and switches,
another network was prepared for control and management. This network
connects compute nodes, SDN-enabled switches and the SDN controller.
The interaction between the SDN controller and SDN switches is performed
via OpenFlow protocol with this management network. The compute node
that runs MPI's rank 0 process and the SDN controller also communicates
with this network. Other compute nodes were connected to the
management network as well, but those connections were not used in this
experiment.

CentOS 6.4 was installed on all computers including the compute nodes and
SDN controller. The SDN controller was developed using a SDN controller
framework Trema~\autocite{trema} 0.4.6 and Ruby 1.9.3. The SDN
MPI Library and the benchmark application were written in C and
compiled with gcc 4.4.7. As a representative of a conventional MPI,
Open MPI~\autocite{Gabriel2004} 1.5.4 was used.

\begin{figure}
    \centering
    \includegraphics{sdn-mpi-exp-env}
    \caption{Experimental Environment}%
    \label{fig:experiment-environment}
\end{figure}

\subsection{Measurement Result}

A micro-benchmark that repeats MPI\_Allreduce 20
times and measures the average execution time of the function was used
for comparing the execution time of the proposed MPI\_Allreduce
with its Open MPI counterpart.

Figure~\ref{fig:evaluation-8nodes} shows the measurement results using 8
nodes, where the horizontal axis indicates the message size and the
vertical axis shows the average time taken to execute
MPI\_Allreduce. The solid line and dashed line represent the
execution time of the proposed framework and Open MPI implementation,
respectively. Figure~\ref{fig:evaluation-8nodes-normalized} shows the
speedup of the proposed framework in comparison with the Open MPI
implementation. The maximum of performance improvement is 41\%.

\begin{figure}
    \centering
    \includegraphics{allreduce_8nodes}
    \caption{Comparison of Execution Time of MPI\_Allreduce (8 compute nodes)}%
    \label{fig:evaluation-8nodes}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{allreduce_8nodes_speedup}
    \caption{Speedup of Proposed MPI\_Allreduce (8 compute nodes)}%
    \label{fig:evaluation-8nodes-normalized}
\end{figure}

Figure~\ref{fig:evaluation-16nodes} shows the result of using 16 nodes.
Figure~\ref{fig:evaluation-16nodes-normalized} indicates the speedup of the
proposed MPI\_Allreduce. It shows how the proposed framework reduces the
execution time of MPI\_Allreduce for 56\% at most.

\begin{figure}
    \centering
    \includegraphics{allreduce_16nodes}
    \caption{Comparison of Execution Time of MPI\_Allreduce (16 compute nodes)}%
    \label{fig:evaluation-16nodes}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{allreduce_16nodes_speedup}
    \caption{Speedup of Proposed MPI\_Allreduce (16 compute nodes)}%
    \label{fig:evaluation-16nodes-normalized}
\end{figure}

\section{Related Work}\label{sec:iii-related-work}

There have been many research works related to MPI\@. Since MPI is merely a
specification for standard APIs for parallel programming, several algorithms
for collective operations have been proposed and implemented targeting several
network technology. As a representative example of such works,
MVAPICH~\autocite{mvapich} can be raised. MVAPICH is an MPI implementation
targeting InfiniBand, which most of high-performance computing systems ranked
in TOP500 has adopted. Sur \emph{et al.}~\autocite{Sur2011} designed MPI
library by leveraging novel InfiniBand-offered features.
They explored new architectures from system point of view and new programming
paradigms from application point of view for keep scaling out applications on
more powerful computing systems. Jiuxing \emph{et al.}~\autocite{Jiuxing2004}
also investigated MPI communication protocol focusing on RDMA operations in
InfiniBand. Our approach has some points in common in terms that our research
also aims to benefit from the features of the underlying network, while our
work differs from that we leverage Software-Defined Networking instead of
InfiniBand from the purpose of investigating the feasibility of dynamic
control of network from application point of view.

Researchers have attempted to elaborate algorithms for MPI collective
operations. Optimized algorithms have been proposed for
MPI\_Alltoall~\autocite{Bruck1997},
MPI\_Reduce\_scatter~\autocite{Iannello1997}, MPI\_Reduce and MPI\_Allreduce.
Most of the optimized algorithms are specialized either for latency or
throughput, so switching multiple algorithms depending on the message size or
process number makes the MPI implementation behave faster for various message
sizes and process numbers~\autocite{Thakur2005}.

Another approach is to offload the communication or computation of collective
communication operations to hardware. For example, the
\emph{K-computer}~\autocite{Yokokawa2011} has a hardware module called
\emph{Tofu Barrier Interface}~\autocite{Ajima2012} on all nodes. This module
executes Barrier, Broadcast, Reduce and Allreduce collective operations in
hardware instead of software.

Past works including existing MPI implementations have been successful
in switching between multiple algorithms depending on message size, node
number, \emph{etc.} to accelerate collective communication of MPI\@. Using
such a mechanism, estimation of the threshold value for parameters like
message size, node number, \emph{etc.} is essential. Pje\v{s}ivac-Grbovi\'{c}
\emph{et al.}~\autocite{PjesivacGrbovic2007} compared several parallel
communication models that are frequently used for dynamically estimating the
threshold values. In contrast, our current implementation always uses
recursive doubling for executing MPI\_Allreduce. Therefore, our system has a
disadvantage in the cases where small data size is treated on MPI\_Allreduce
and thus the latency is more respected than the bandwidth. For this
disadvantage, we introduce automatic algorithm switching leveraging estimation
models mentioned in the work of Pje\v{s}ivac-Grbovi\'{c} \emph{et al.},
depending on the situations.

The Fabric Collective Accelerator (FCA)~\autocite{fca} is a product by
Mellanox Technologies with the target of accelerating collective
communication on clusters with InfiniBand interconnect. FCA accelerates
collective communication by offloading computations to an InfiniBand
Host Channel Adapter (HCA). It also optimizes communication flow
according to job and topology. FCA optimizes collective tree and rank
placement to control communication flow. In contrast, our proposed
framework is capable of adaptively reconfiguring the network itself,
which is more flexible. FCA also requires InfiniBand hardware, but we
focus on a commodity Ethernet network.

Furthermore, there have been many research reports focusing on adaptive
use of networks for high-performance computing. Geoffray \emph{et
al.}~\autocite{Geoffray2008} proposed an adaptive routing method on Myrinet
and the above mentioned literature~\autocite{Jiuxing2004} explored the
adaptive use of multiple independent networks on InfiniBand. Our research also
aims for a dynamic use of the underlying interconnection network, but differs
in that we attempted to use a different interconnection network.

\section{Conclusion}\label{sec:iii-conclusion}

In this chapter, have attempted to reduce the execution time of MPI
collectives by dynamically controlling the packet flow in the interconnect
using Software-Defined Networking~(SDN).
By using SDN, we have proposed a novel framework for accelerating collectives
that can effectively make use of redundant routes by having SDN interact
with the communication pattern of collectives. We have designed and
implemented a system of three modules cooperating together to realize
our proposed framework. The evaluation showed that the proposed framework
speeds up the execution time of MPI\_Allreduce for 56\% at most compared with
a conventional implementation of MPI\_Allreduce. As a result, we have
confirmed the superiority of the proposed framework over conventional methods.

Several issues to tackle have remained for the realization of practical
and useful collective acceleration framework leveraging SDN\@:

The first issue is lack of support for multiple processes on a single compute
node. On modern computing platforms where multiple cores are implemented in a
single compute node, the assumption set in this preliminary stage of our
research is neither practical nor realistic. Therefore, by integrating
kernel-assisted communication such as KNEM~\autocite{Goglin2013} or Cross
Memory Attach~(CMA) with our proposed method, we need to consider intra-node
communication for pure MPI jobs. Also, computing paradigms such as the hybrid
use of OpenMP with MPI must be considered for enhancing practicality. Also,
other implementations of MPI such as MVAPICH~\autocite{mvapich} are essential
for investigating the practicality and usefulness of SDN\@.

The second issue regards the necessity of additional experiments on a
larger scale environment. In this chapter, we have verified the
feasibility and possibility of the proposed framework
through the experiments using a simple prototypic implementation.
However, because OpenFlow switches are expensive and thus only a small
size cluster was available for us, we could only conduct
a small number of experiments in a small cluster environment. Therefore,
after succeeding with a more sophisticated implementation of the
proposed framework we understand that further experiments
on a larger scale environment are essential for the evaluation of
scalability.

The third issue is scalability issue caused by the SDN controller.
Since our current implementation requires communication between MPI
processes and SDN controller and route generation for each MPI
communication request, the SDN controller might become a scalability
bottleneck on larger environments. Therefore, we are planning to cache IP
addresses and MPI rank numbers for each process in the SDN controller.
Furthermore, we enhance our implementation so that only the root process
interacts with the SDN controller and conveys information of all participating
processes in the collective communication.
